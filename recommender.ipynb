{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "import re\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from nltk.stem import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 11352 terms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model with i=24\nFound best model:\n<class 'sklearn.decomposition.online_lda.LatentDirichletAllocation'>\n0.22383237055\nRecommending...\ndone\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stem_list = lambda x: [stemmer.stem(y) for y in x]\n",
    "class NewsRecommender:\n",
    "    \"\"\"\n",
    "    обучить систему на корпусе текстов  с помощью тематической модели и метрики, выбранных в результате исследования\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.vectorizer = TfidfVectorizer(min_df=0.1, max_df=0.5)\n",
    "        self.w2v = None\n",
    "        self.texts = None\n",
    "        self.textmatrix = None\n",
    "        self.neighbors = NearestNeighbors(metric=pairwise.cosine_distances)\n",
    "\n",
    "    def train(self, texts):\n",
    "        self.texts = texts.data\n",
    "        w2v_path = Path(\"w2v-model.bin\")\n",
    "        stemmed_texts = [\" \".join(stem_list(x.split())) for x in self.texts]\n",
    "        topic_range = range(4, 25)\n",
    "        tfidfs = self.vectorizer.fit_transform(stemmed_texts)\n",
    "        fnames = self.vectorizer.get_feature_names()\n",
    "        if not w2v_path.is_file():\n",
    "            docgen = TokenGenerator(stemmed_texts, [])\n",
    "            w2v_model = Word2Vec(docgen, size=500, min_count=20, sg=1)\n",
    "        else:\n",
    "            w2v_model = Word2Vec.load(\"w2v-model.bin\")\n",
    "\n",
    "        self.w2v = w2v_model\n",
    "        print(\"Model has %d terms\" % len(w2v_model.wv.vocab))\n",
    "        w2v_model.save(\"w2v-model.bin\")\n",
    "        # texts = fetch_20newsgroups()\n",
    "        best_model = None\n",
    "        best_score = 0\n",
    "        best_matrix = None\n",
    "\n",
    "        for i in topic_range:\n",
    "            lda_name = Path(\"lda_%d.pkl\" % i)\n",
    "            nmf_name = Path(\"nmf_%d.pkl\" % i)\n",
    "            if not lda_name.is_file():\n",
    "                lda = LatentDirichletAllocation(n_components=i, learning_method=\"batch\")\n",
    "                W_lda = lda.fit_transform(tfidfs)\n",
    "                with open(lda_name, \"wb\") as io:\n",
    "                    pickle.dump(lda, io)\n",
    "            else:\n",
    "                with open(lda_name, \"rb\") as io:\n",
    "                    lda = pickle.load(io)\n",
    "                    W_lda = lda.transform(tfidfs)\n",
    "\n",
    "            H_lda = lda.components_\n",
    "            if not nmf_name.is_file():\n",
    "                nmf = NMF(n_components=i, init=\"nndsvda\")\n",
    "                W_nmf = nmf.fit_transform(tfidfs)\n",
    "                with open(nmf_name, \"wb\") as io:\n",
    "                    pickle.dump(nmf, io)\n",
    "            else:\n",
    "                with open(nmf_name, \"rb\") as io:\n",
    "                    nmf = pickle.load(io)\n",
    "                    W_nmf = nmf.transform(tfidfs)\n",
    "\n",
    "\n",
    "            H_nmf = nmf.components_\n",
    "            term_rankings_lda = [get_descriptor(fnames, H_lda, x, 10) for x in range(0, i)]\n",
    "            term_rankings_nmf = [get_descriptor(fnames, H_nmf, x, 10) for x in range(0, i)]\n",
    "            lda_score = tcw2c(self.w2v, term_rankings_lda)\n",
    "            nmf_score = tcw2c(self.w2v, term_rankings_nmf)\n",
    "\n",
    "            if lda_score > nmf_score and lda_score > best_score:\n",
    "                best_score = lda_score\n",
    "                best_model = lda\n",
    "                best_matrix = W_lda\n",
    "            elif nmf_score > lda_score and nmf_score > best_score:\n",
    "                best_score = nmf_score\n",
    "                best_model = nmf\n",
    "                best_matrix = W_nmf\n",
    "            print(\"Evaluated model with i=\"+str(i))\n",
    "        print(\"Found best model:\")\n",
    "        self.model = best_model\n",
    "        self.textmatrix = best_matrix\n",
    "        print(type(best_model))\n",
    "        #print(best_model.n_components_)\n",
    "        print(best_score)\n",
    "\n",
    "    \"\"\"\n",
    "    выдать k самых пожих новостей для заданного заголовка по функции расстояния, выбранной в результате исследования\n",
    "    обратите внимание, что text_sample может содержать слова не из обучающего корпуса\n",
    "    \"\"\"\n",
    "\n",
    "    def recommend(self, text_sample, k):\n",
    "        print(\"Recommending...\")\n",
    "        sample_vecspace = self.vectorizer.transform([\" \".join(stem_list(text_sample.split()))])\n",
    "        reduced_vecspace = self.model.transform(sample_vecspace)\n",
    "        distances = pairwise.cosine_distances(reduced_vecspace, self.textmatrix)\n",
    "        dist_list = []\n",
    "        for i in range(len(distances[0])):\n",
    "            dist_list.append((i , distances[0][i]))\n",
    "        dist_list = sorted(dist_list, key=lambda x: x[1])\n",
    "        \n",
    "\n",
    "        return [(x[0], self.texts[x[0]]) for x in dist_list[:k]]\n",
    "\n",
    "\n",
    "def tcw2c(w2v_model, term_rankings):\n",
    "    overall_coherence = 0.0\n",
    "    for topic_index in range(len(term_rankings)):\n",
    "        pair_scores = []\n",
    "        for pair in combinations(term_rankings[topic_index], 2):\n",
    "            try:\n",
    "                pair_scores.append( w2v_model.similarity(pair[0], pair[1]) )\n",
    "            except KeyError:\n",
    "                continue\n",
    "        topic_score = sum(pair_scores) / len(pair_scores)\n",
    "        overall_coherence += topic_score\n",
    "    return overall_coherence / len(term_rankings)\n",
    "\n",
    "\n",
    "def get_descriptor(terms, H, topic_index, top):\n",
    "    top_indices = np.argsort(H[topic_index, :])#[::-1]\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append(terms[term_index])\n",
    "    return top_terms\n",
    "\n",
    "\n",
    "class TokenGenerator:\n",
    "    def __init__(self, documents, stopwords):\n",
    "        self.documents = documents\n",
    "        self.stopwords = stopwords\n",
    "        self.tokenizer = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        for doc in self.documents:\n",
    "            tokens = []\n",
    "            for tok in self.tokenizer.findall(doc):\n",
    "                if tok in self.stopwords:\n",
    "                    tokens.append(\"<stopword>\")\n",
    "                elif len(tok) >= 2:\n",
    "                    tokens.append(tok)\n",
    "            yield tokens\n",
    "\n",
    "\n",
    "rec = NewsRecommender()\n",
    "texts = fetch_20newsgroups()\n",
    "rec.train(texts)\n",
    "recs = rec.recommend(\"\"\"\n",
    "Nelson was born and raised in Salt Lake City, Utah. He attended the University of Utah for his undergraduate and medical school education, then did further surgical training and earned a Ph.D. at the University of Minnesota. He served for two years in the U.S. Army Medical Corps during the Korean War, then did additional surgical training at Harvard Medical School via Massachusetts General Hospital. In 1955, Nelson returned to Salt Lake City and accepted a professorship at the University of Utah School of Medicine. Nelson spent the next 29 years at Utah working in the field of cardiothoracic surgery and serving in a variety of LDS Church leadership positions, beginning locally and then as the LDS Church's Sunday School General President from 1971 to 1979.[5] Nelson became a renowned heart surgeon, and served as president of the Society for Vascular Surgery and the Utah Medical Association.[6]\"\"\"\n",
    "                     , 5)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4967\nFrom: lieuwen@allegra.att.com (Dan Lieuwen)\nSubject: Re: The obvious isn't politically correct.\nOrganization: AT&T Bell Laboratories, Murray Hill, NJ\nLines: 5\n\nThe last state church was in Massachusetts.  Sam Adams, the patriot-brewmaster,\nduring his tenure as governor after the Revolutionary War got it passed.\nI believe it was eliminated around 1820.\n\ndan\n\n==============================================\n7498\nFrom: halat@pooh.bears (Jim Halat)\nSubject: Re: That silly outdated Bill (was Re: Koresh and Miranda)\nReply-To: halat@pooh.bears (Jim Halat)\nLines: 9\n\nIn article <1993Apr14.165633.2170@cbnews.cb.att.com>, lvc@cbnews.cb.att.com (Larry Cipriani) writes:\n>As for the MOVE incident, wasn't the mayor of Philadelphia at the time Black ?\n\nFor the first Move incident (no bomb, several members killed in\ngunfire, circa 1978) the mayor was the very white Frank Rizzo.  \nFor the second (bomb included) the mayor was Wilson Goode, who \nis indeed black.\n\n-jim halat\n\n==============================================\n9817\nFrom: jmk@cbnews.cb.att.com (joseph.m.knapp)\nSubject: Re: Biblical Backing of Koresh's 3-02 Tape (Cites enclosed)\nOrganization: AT&T\nLines: 9\n\ncotera@woods.ulowell.edu writes:\n>           David Thibedeau (sp?), one of the cult members, said that the fire\n>was started when one of the tanks spraying the tear gas into the facilities\n>knocked over a lantern.\n\nSort of a \"Mrs. O'Leary's\" tank theory? Moooo.\n\n---\nJoe Knapp   jmk@cbvox.att.com\n\n==============================================\n8952\nFrom: PETCH@gvg47.gvg.tek.com (Chuck)\nSubject: Daily Verse\nLines: 6\n\nAnd the Lord's servant must not quarrel; instead, he must be kind to everyone,\nable to teach, not resentful. Those who oppose him he must gently instruct, in\nthe hope that God will grant them repentance leading them to a knowledge of the\ntruth, and that they will come to their senses and escape from the trap of the\ndevil, who has taken them captive to do his will. \nIITimothy 2:24-26\n\n==============================================\n780\nOrganization: University of Notre Dame - Office of Univ. Computing\nFrom: <RVESTERM@vma.cc.nd.edu>\nSubject: Re: Bosox win again! (the team record is 9-3)\n <1993Apr18.233404.16702@ncar.ucar.edu>\nLines: 12\n\nIn article <1993Apr18.233404.16702@ncar.ucar.edu>, amj@rsf.atd.ucar.edu (Anthony\nMichael Jivoin) says:\n>\n>With the \"HAWK\", the Red Sox definitely have a chance for the\n>east this year. He brings class, work ethic and leadership to\n>the park each day.\n>\n\ntoo bad he doesn't bring the ability to hit, pitch, field or run.\n\nbob vesterman.\n\n\n==============================================\n"
     ]
    }
   ],
   "source": [
    "for rec in recs:\n",
    "    print(rec[0])\n",
    "    print(rec[1])\n",
    "    print('==============================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

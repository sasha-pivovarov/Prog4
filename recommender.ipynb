{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "import re\n",
    "from itertools import combinations\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsRecommender:\n",
    "    \"\"\"\n",
    "    обучить систему на корпусе текстов  с помощью тематической модели и метрики, выбранных в результате исследования\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.w2v = None\n",
    "        self.texts = None\n",
    "        \n",
    "        \n",
    "    def train(self, texts):\n",
    "        \n",
    "        topic_range = range(4, 20)\n",
    "        tfidfs = self.vectorizer.fit_transform(texts)\n",
    "        fnames = self.vectorizer.get_feature_names()\n",
    "        docgen = TokenGenerator( texts, [] )\n",
    "        w2v_model = Word2Vec(docgen, size=500, min_count=20, sg=1)\n",
    "        self.w2v = w2v_model\n",
    "        print( \"Model has %d terms\" % len(w2v_model.wv.vocab) )\n",
    "        w2v_model.save(\"w2v-model.bin\")\n",
    "        # texts = fetch_20newsgroups()\n",
    "        best_model = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for i in topic_range:\n",
    "            lda = LatentDirichletAllocation(n_components=i, learning_method=\"batch\")\n",
    "            W_lda = lda.fit_transform(tfidfs)\n",
    "            with open(\"lda_%d.pkl\" % i, \"w\") as io:\n",
    "                pickle.dump(lda, io)\n",
    "            H_lda = lda.components_\n",
    "            nmf = NMF(n_components=i, solver=\"nndsvda\")\n",
    "            W_nmf = nmf.fit_transform(tfidfs)\n",
    "            with open(\"nmf_%d.pkl\" % i, \"w\") as io:\n",
    "                pickle.dump(nmf, io)\n",
    "            H_nmf = nmf.components_\n",
    "            term_rankings_lda = get_descriptor(fnames, H_lda, i, 10)\n",
    "            term_rankings_nmf = get_descriptor(fnames, H_nmf, i, 10)\n",
    "            lda_score = tcw2c(self.w2v, term_rankings_lda)\n",
    "            nmf_score = tcw2c(self.w2v, term_rankings_nmf)\n",
    "            \n",
    "            if lda_score > nmf_score and lda_score > best_score:\n",
    "                best_score = lda_score\n",
    "                best_model = lda\n",
    "            elif nmf_score > lda_score and nmf_score > best_score:\n",
    "                best_score = nmf_score\n",
    "                best_model = nmf\n",
    "        \n",
    "        self.model = best_model\n",
    "        print(type(best_model))\n",
    "        print(best_model.n_topics)\n",
    "        print(best_score)\n",
    "            \n",
    "\n",
    "            \n",
    "    \"\"\"\n",
    "    выдать k самых пожих новостей для заданного заголовка по функции расстояния, выбранной в результате исследования\n",
    "    обратите внимание, что text_sample может содержать слова не из обучающего корпуса\n",
    "    \"\"\"\n",
    "    def recommend(self, text_sample, k):\n",
    "        return [\"news_1\", \"news_2\", ... , \"news_k\"]\n",
    "    \n",
    "def tcw2c( w2v_model, term_rankings ):\n",
    "    overall_coherence = 0.0\n",
    "    for topic_index in range(len(term_rankings)):\n",
    "        pair_scores = []\n",
    "        for pair in combinations( term_rankings[topic_index], 2 ):\n",
    "            pair_scores.append( w2v_model.similarity(pair[0], pair[1]) )\n",
    "        topic_score = sum(pair_scores) / len(pair_scores)\n",
    "        overall_coherence += topic_score\n",
    "    return overall_coherence / len(term_rankings)\n",
    "\n",
    "def get_descriptor( terms, H, topic_index, top ):\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( terms[term_index] )\n",
    "    return top_terms\n",
    "        \n",
    "    \n",
    "class TokenGenerator:\n",
    "    def __init__( self, documents, stopwords ):\n",
    "        self.documents = documents\n",
    "        self.stopwords = stopwords\n",
    "        self.tokenizer = re.compile( r\"(?u)\\b\\w\\w+\\b\" )\n",
    "\n",
    "    def __iter__( self ):\n",
    "        for doc in self.documents:\n",
    "            tokens = []\n",
    "            for tok in self.tokenizer.findall( doc ):\n",
    "                if tok in self.stopwords:\n",
    "                    tokens.append( \"<stopword>\" )\n",
    "                elif len(tok) >= 2:\n",
    "                    tokens.append( tok )\n",
    "            yield tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
